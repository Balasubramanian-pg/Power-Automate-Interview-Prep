# 23. Validating Data mid flow

## 1. Flow Overview
- **Flow Name**: Mid-Flow Data Integrity and Schema Validation Framework
- **Business Problem Statement**: Downstream systems often fail or ingest "garbage data" because incoming payloads from external sources (APIs, CSVs, or Forms) do not meet strict schema requirements or business logic rules.
- **Business Impact / Value**: Reduces manual data cleanup by 90%, prevents downstream system crashes, ensures high data quality for reporting, and provides immediate feedback to data providers when submissions are invalid.
- **Trigger Type**: Automated
- **Trigger Source**: Inbound Data Source (e.g., HTTP Request, File Drop, or Message Queue)
- **Systems / Connectors Involved**: Source System (e.g., Salesforce, Forms), Validation Engine (iPaaS), Target System (e.g., ERP, SQL Database), Notification System (e.g., Email, Teams).
- **Expected Run Frequency**: High (Real-time or Batch)
- **Estimated Data Volume**: Variable; optimized for both single-record transactions and bulk processing.

## 2. Trigger Design
- **Trigger Connector & Action**: Typically an "On New Item" or "When an HTTP Request is Received" action.
- **Why This Trigger Was Chosen**: To intercept data at the moment of entry before any processing or transformation occurs.
- **Trigger Conditions Used**: Yes
- **Trigger Condition Logic**: `@not(empty(triggerOutputs()?['body']?['RequiredField']))` — used to filter out completely empty or malformed payloads at the platform level to save on run costs.
- **Polling vs Event-Based**: Event-Based for immediate validation.
- **How Unnecessary Runs Are Avoided**: Using trigger conditions to ensure the payload contains at least the minimum required identifiers before the flow initializes.

## 3. End-to-End Flow Narrative
The flow begins when data is received from a source system. Instead of immediately pushing this data to the destination, the flow enters a "Validation Zone."

1.  **Schema Validation**: The flow first attempts to parse the data against a predefined JSON schema. If the structure is incorrect (e.g., a string where an integer should be), the flow catches the error immediately.
2.  **Business Logic Check**: The flow then checks for specific business rules that a schema cannot catch, such as "Is the Discount Percentage greater than 50%?" or "Does the Customer ID exist in our Master Data?"
3.  **Decision Point**: If all validations pass, the flow proceeds to the "Success Path" to update the target systems.
4.  **Exception Path**: If any validation fails, the flow enters the "Exception Path," where the error is logged, and a notification is sent back to the source or an administrator with specific details on why the data was rejected.

> [!IMPORTANT]
> Validating data mid-flow follows the "Fail Fast" principle. It is more efficient to fail a run in the first 5 seconds than to let it run for 2 minutes only to fail at the final database insert.

## 4. Key Actions and Connectors
Document only the actions that influence logic, performance, or reliability.

| Action Name | Connector | Purpose | Key Inputs | Key Outputs | Why This Action |
|------------|-----------|---------|------------|-------------|----------------|
| Parse JSON | Data Operations | Validates payload against a strict schema. | Body of the trigger | Structured Data | Automatically fails the action if the schema doesn't match. |
| Filter Array | Data Operations | Checks for missing or invalid items in a collection. | Array of records | Filtered (Invalid) items | Efficiently identifies bad records without looping. |
| Condition | Control | Branches logic based on validation results. | Boolean expression | True/False path | Directs the flow to Success or Failure paths. |
| Compose (Error Log) | Data Operations | Aggregates all validation errors into a single string. | Error messages | Final Error Report | Provides a clean summary for notifications. |

## 5. Control Logic and Flow Structure
- **Conditions Used**: Used to check the results of `Filter Array` (e.g., `length(body('Filter_Array'))` is greater than 0).
- **Switch Statements**: Used when data must be validated differently based on a "Category" or "Region" field.
- **Loops (Apply to each / Do until)**: Minimized for performance; used only when individual record-level feedback is required for a batch.
- **Nested Loops**: No. Nested loops are avoided to prevent exponential increases in execution time; `Filter Array` is used instead.
- **Parallel Branches**: Used to perform multiple independent validations simultaneously (e.g., checking a CRM and an ERP at the same time) to reduce total latency.
- **Scope Usage**: Critical. "Validation Scope," "Success Scope," and "Failure Scope" are used to organize logic and manage error handling.

## 6. Error Handling and Monitoring
- **Anticipated Failure Scenarios**: Schema mismatch, null values in non-nullable fields, lookup values not found in reference tables, and data type overflows.
- **Try Scope Logic**: Contains the main validation actions and the primary data processing.
- **Catch Scope Logic**: Configured to "Run After" the Try Scope has failed. It captures the error message and formats the rejection notification.
- **Finally Scope Logic**: Used to log the final status (Success/Failure) to a centralized audit log regardless of the outcome.
- **Run After Configuration**: The "Failure Notification" action is set to run only if the "Validation Scope" fails or is skipped.
- **Failure Notification Method**: Adaptive Cards in Microsoft Teams or an automated email to the data submitter.
- **Logging Strategy**: Every validation failure is logged to a SharePoint list or SQL table with the Correlation ID, Timestamp, and specific Error Reason.
- **How to Debug a Failed Run**: Check the `Parse JSON` output for schema errors or the `Filter Array` output to see which specific records failed the business logic check.

> [!TIP]
> Use the `result()` expression on a Scope to get an array of all actions within that scope and filter for those with a status of 'Failed'. This allows you to programmatically identify exactly which validation step failed.

## 7. Data Handling and Expressions
- **Variables Used**: `varValidationErrors` (Array), `varIsValid` (Boolean).
- **Key Expressions**:
    - `empty(triggerOutputs()?['body']?['Field'])`: Checks for nulls.
    - `if(contains(body('Check_Data'), 'Error'), false, true)`: Inline validation logic.
    - `coalesce(outputs('Action')?['body/Value'], 'N/A')`: Handles nulls by providing a default value.
- **Data Operations (Select / Filter array / Compose)**: `Filter Array` is the primary tool for validating batches without the overhead of a loop.
- **Why Expressions Were Used Instead of Actions**: Expressions like `coalesce` and `empty` are used inside `Compose` actions to reduce the number of "Condition" steps, making the flow cleaner and faster.

## 8. Performance and Scalability
- **Known Bottlenecks**: Large `Apply to Each` loops during validation.
- **Loop Optimization Strategy**: Enable "Concurrency Control" on loops if the validation steps do not have dependencies on each other.
- **Pagination Handling**: If validating against a reference list (e.g., checking if a Product ID exists), ensure pagination is enabled on the "Get Items" action to handle more than 100-500 records.
- **Concurrency Control**: Set to 20-50 for high-volume validation tasks.
- **What Breaks at Higher Data Volumes**: Single-threaded loops will cause the flow to time out. API limits (Request Limits) may be hit if every validation requires an external lookup.
- **Redesign Approach for Scale**: Move complex business logic validation to a dedicated "Validation API" or a Database Stored Procedure if the logic exceeds 10-15 conditions.

## 9. Security and Governance
- **Connection Type (Personal / Service Account)**: Service Account (Principal) to ensure continuity and centralized permissions.
- **Environment Strategy**: Developed in a "Sandbox" environment and promoted to "Production" via Managed Solutions.
- **Secrets Handling**: Any API keys or connection strings used for validation lookups are stored in Azure Key Vault.
- **DLP Considerations**: Ensure the "Data Operations" and "Notification" connectors are in the same data group (Business vs. Non-Business).
- **Access Control Notes**: Only the integration team has access to the run history, as it may contain sensitive PII being validated.

## 10. Testing and Validation
- **Test Scenarios Covered**:
    - Happy Path: All data is correct.
    - Schema Failure: Missing required fields.
    - Logic Failure: Values outside of acceptable ranges (e.g., negative price).
    - Reference Failure: ID does not exist in the target system.
- **Edge Cases Considered**: Empty arrays, special characters in strings, and maximum string length violations.
- **Failure Testing**: Manually injecting "Bad JSON" to ensure the `Catch` block triggers correctly.
- **Rerun / Recovery Strategy**: Flows that fail validation are NOT retried automatically. They require the source data to be corrected and resubmitted to maintain a clean audit trail.

## 11. Interview Question Mapping
- **Explain This Flow in 2–3 Minutes**: This flow acts as a "Quality Gate." It triggers when data is received, passes it through a `Parse JSON` action for structural integrity, and uses `Filter Array` to check business rules. If any check fails, it alerts the user; otherwise, it commits the data to the database.
- **How Failures Are Handled**: We use a Try/Catch pattern using Scopes. If any validation action fails, the Catch scope triggers, sends a detailed error report, and terminates the flow as "Succeeded" (to avoid false-positive alerts) but with a "Rejected" status in the logs.
- **How Performance Is Optimized**: By avoiding loops. We use `Filter Array` to find invalid records in a batch and `Trigger Conditions` to prevent the flow from even starting if the payload is fundamentally broken.
- **One Trade-Off Made**: We chose to use a strict JSON schema. While this makes the flow less flexible to source system changes, it significantly increases the reliability of the downstream ERP system.

## 12. Lessons Learned
- **Initial Issues**: Initially, we used individual "Condition" actions for every field, which made the flow unreadable and slow.
- **Improvements Made**: Switched to `Parse JSON` for structural validation and a single `Filter Array` for business logic, which reduced the action count by 60%.
- **What I Would Do Differently Now**: I would implement a "Validation Rules" table in a database, allowing business users to update thresholds (like "Max Discount") without needing to edit the flow logic itself.

> [!WARNING]
> Always ensure that your `Parse JSON` schema is updated if the source system adds new "Required" fields, otherwise, the flow will begin rejecting valid traffic.