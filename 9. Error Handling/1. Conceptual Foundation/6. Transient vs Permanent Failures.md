# 6. Transient vs Permanent Failures

## 1. Flow Overview
- **Flow Name**: Resilient Error Classification and Handling Framework
- **Business Problem Statement**: Automated systems often fail due to external factors. Treating all failures identically leads to either unnecessary manual overhead (for temporary glitches) or infinite retry loops and resource exhaustion (for logic errors).
- **Business Impact / Value**: Reduces manual intervention by 70-90% through automated retries of transient issues, while ensuring data integrity by immediately flagging permanent issues for human review.
- **Trigger Type**: Automated (Sub-flow / Child Flow pattern)
- **Trigger Source**: API Request or Internal System Exception
- **Systems / Connectors Involved**: HTTP Request, Cloud Logging (Azure Monitor/CloudWatch), Notification Services (Teams/Slack/Email), Database/ERP.
- **Expected Run Frequency**: High (invoked on every critical transaction failure).
- **Estimated Data Volume**: Dependent on parent flow; typically lightweight metadata (Error Code, Message, Stack Trace).

## 2. Trigger Design
- **Trigger Connector & Action**: HTTP Webhook / "When an HTTP request is received"
- **Why This Trigger Was Chosen**: Decoupling error handling into a centralized "Error Handler" flow allows for consistent logic across multiple business processes.
- **Trigger Conditions Used**: No
- **Trigger Condition Logic**: N/A
- **Polling vs Event-Based**: Event-Based (Triggered immediately upon failure detection in a parent process).
- **How Unnecessary Runs Are Avoided**: The parent flow performs a preliminary check; only failures that cannot be resolved by local "Retry Policies" are passed to this classification framework.

## 3. End-to-End Flow Narrative
The flow begins when a parent process encounters an exception that exceeds its local retry limit or is outside its immediate scope. 

1. **Classification**: The flow receives the error payload and inspects the status code or error message.
2. **Decision Point (Transient vs. Permanent)**: 
    - **Transient**: If the error is a 429 (Too Many Requests), 502 (Bad Gateway), 503 (Service Unavailable), or 504 (Gateway Timeout), it is classified as **Transient**.
    - **Permanent**: If the error is a 400 (Bad Request), 401 (Unauthorized), 403 (Forbidden), or 404 (Not Found), it is classified as **Permanent**.
3. **Transient Path**: The flow initiates an Exponential Backoff strategy. It waits for a calculated duration and then signals the parent flow to retry or attempts the operation itself.
4. **Permanent Path**: The flow halts execution, logs the incident in a centralized database, and sends a high-priority alert to the technical team with the specific payload that caused the failure.
5. **Termination**: The flow ends by returning a "Success" (if transient retry succeeded) or "Failed" (if permanent or retries exhausted) status to the monitoring dashboard.

## 4. Key Actions and Connectors
Documenting the logic-heavy components of the classification engine.

| Action Name | Connector | Purpose | Key Inputs | Key Outputs | Why This Action |
|------------|-----------|---------|------------|-------------|----------------|
| Parse_Error_Payload | Data Operations | Extracts status codes and error messages. | `triggerBody()` | `StatusCode`, `ErrorMessage` | Essential for routing logic. |
| Check_Failure_Type | Control | Determines the branch (Transient vs Permanent). | `StatusCode` | Boolean (IsTransient) | Central logic gate for the entire flow. |
| Exponential_Wait | Delay | Prevents "thundering herd" problems during service outages. | `2 ^ RetryCount` | N/A | Standard industry practice for transient recovery. |
| Log_Permanent_Failure | SQL / Dataverse | Records the failure for audit and debugging. | `Payload`, `ErrorDetails` | `RecordID` | Ensures no data is lost when manual intervention is needed. |

## 5. Control Logic and Flow Structure
- **Conditions Used**: A primary condition checks if the error code exists in a predefined "Transient List" (e.g., 429, 500, 502, 503, 504).
- **Switch Statements**: Used to handle specific permanent errors differently (e.g., a 401 triggers an auth-refresh flow, while a 400 triggers a data-correction alert).
- **Loops (Apply to each / Do until)**: A "Do until" loop is used for the retry mechanism, incrementing a counter until a maximum threshold is reached.
- **Nested Loops**: No, to maintain performance and readability.
- **Parallel Branches**: Used to simultaneously log to a database and send a notification for permanent failures.
- **Scope Usage**: "Try-Catch" blocks are used within the retry loop to capture subsequent failures during the retry attempt.

## 6. Error Handling and Monitoring
- **Anticipated Failure Scenarios**: Exhaustion of retry attempts, notification service downtime, or invalid error payloads.
- **Try Scope Logic**: Contains the primary retry attempt logic.
- **Catch Scope Logic**: Executes if the retry attempt fails again; determines if another retry is allowed or if it should be escalated to "Permanent."
- **Finally Scope Logic**: Updates the master transaction log with the final status (Resolved/Failed).
- **Run After Configuration**: The "Catch" block is set to run only if the "Try" block fails or times out.
- **Failure Notification Method**: Adaptive Cards in Microsoft Teams or PagerDuty integration for Permanent failures.
- **Logging Strategy**: Every attempt is logged with a timestamp, attempt number, and response received.
- **How to Debug a Failed Run**: Review the "Parse_Error_Payload" output to see exactly what the external system returned.

> [!IMPORTANT]
> Never retry a **Permanent Failure** (4xx). Doing so wastes compute resources and can lead to account lockout or IP blacklisting in the case of 401/403 errors.

## 7. Data Handling and Expressions
- **Variables Used**: `varRetryCount` (Integer), `varIsTransient` (Boolean), `varWaitInterval` (Integer).
- **Key Expressions**: 
    - `contains(createArray(429, 502, 503, 504), body('Parse_Error')?['status'])` to identify transient types.
    - `mul(outputs('Base_Delay'), range(1, 5))` for calculating backoff.
- **Data Operations (Select / Filter array / Compose)**: "Compose" is used to build the final JSON notification payload.
- **Why Expressions Were Used Instead of Actions**: Expressions are used for status code checks to reduce the number of visual steps in the flow, making it easier to maintain.

## 8. Performance and Scalability
- **Known Bottlenecks**: High-frequency retries can hit API rate limits of the automation platform itself.
- **Loop Optimization Strategy**: Using a "Do Until" with a clear exit condition and a maximum limit of 3-5 retries.
- **Pagination Handling**: N/A for this specific logic flow.
- **Concurrency Control**: Set to a high limit to allow multiple failures from different parent flows to be processed simultaneously.
- **What Breaks at Higher Data Volumes**: If a downstream system has a massive outage, thousands of "Transient" retry flows may trigger, potentially hitting platform request limits.
- **Redesign Approach for Scale**: Move to a message queue (like Azure Service Bus or AWS SQS) to buffer error handling requests.

## 9. Security and Governance
- **Connection Type (Personal / Service Account)**: Service Account (Principal) with "Least Privilege" access to logging and notification systems.
- **Environment Strategy**: Deployed in a "Shared Services" environment accessible by all production business units.
- **Secrets Handling**: API keys and connection strings are retrieved from a secure Vault (e.g., Azure Key Vault).
- **DLP Considerations**: Ensure that the "Error Payload" logged does not contain PII (Personally Identifiable Information) or credentials.
- **Access Control Notes**: Only the DevOps and Support teams have "Read" access to the error logs.

> [!WARNING]
> Ensure that "Secure Inputs" and "Secure Outputs" are enabled on actions that handle sensitive data during the error classification process.

## 10. Testing and Validation
- **Test Scenarios Covered**: 
    - Mocking a 503 error to verify the retry loop.
    - Mocking a 400 error to verify immediate escalation.
    - Mocking a 429 error to verify exponential backoff timing.
- **Edge Cases Considered**: Empty error bodies, non-standard status codes (e.g., 418), and network timeouts where no status code is returned.
- **Failure Testing**: Simulating a failure of the notification system itself to ensure the flow still logs to the database.
- **Rerun / Recovery Strategy**: Permanent failures can be manually resubmitted from the "Permanent Failure Log" once the underlying data or logic issue is resolved.

## 11. Interview Question Mapping
- **Explain This Flow in 2â€“3 Minutes**: This is a centralized error-handling framework that distinguishes between transient and permanent failures. It uses status code logic to decide whether to automatically retry an operation (transient) or alert a human (permanent), ensuring system resilience without creating infinite loops.
- **How Failures Are Handled**: We use a Try-Catch-Finally pattern. Transient errors (like 503s) enter a "Do Until" loop with exponential backoff. Permanent errors (like 400s) bypass retries and trigger immediate logging and notifications.
- **How Performance Is Optimized**: By decoupling this logic into a child flow, we keep parent flows lean. We use expressions for classification to minimize action overhead and implement concurrency controls to handle spikes in error volume.
- **One Trade-Off Made**: We chose a maximum of 5 retries. While more retries might catch longer outages, it risks exhausting the automation platform's daily request limits and delaying the eventual notification to the support team.

## 12. Lessons Learned
- **Initial Issues**: Initially, we retried on all 5xx errors, but found that some 500 errors were actually permanent logic bugs in the target API that would never resolve without a code fix.
- **Improvements Made**: Added a "Max Retry" limit and integrated a "Circuit Breaker" pattern where if the same error occurs 50 times across different instances, the flow stops retrying and alerts the team of a systemic outage.
- **What I Would Do Differently Now**: I would implement a "Dead Letter Queue" approach from the start to better manage high volumes of permanent failures during major system migrations.

> [!TIP]
> Always log the "Correlation ID" from the parent flow. It is the only way to effectively trace a failure across multiple decoupled systems.