# 8. Service Throttling Basics

## 1. Flow Overview
- **Flow Name**: Resilient Service Integration Pattern (Throttling Management)
- **Business Problem Statement**: High-volume integrations often exceed the destination service's API limits, resulting in "429 Too Many Requests" errors, data loss, and inconsistent system states.
- **Business Impact / Value**: Ensures system stability, prevents service suspension by providers, guarantees data delivery, and optimizes resource utilization costs.
- **Trigger Type**: Automated
- **Trigger Source**: Upstream Data Source (e.g., Database, Message Queue, or Webhook)
- **Systems / Connectors Involved**: HTTP/REST Connectors, State Management Store (e.g., Redis or SQL), Notification Service.
- **Expected Run Frequency**: High-frequency (Real-time or near real-time).
- **Estimated Data Volume**: 10,000 to 1,000,000+ requests per day depending on service tier.

## 2. Trigger Design
- **Trigger Connector & Action**: Webhook / HTTP Listener or Message Queue Trigger.
- **Why This Trigger Was Chosen**: Event-based triggers allow the system to react immediately to incoming data while providing a mechanism to buffer requests if the downstream service is throttled.
- **Trigger Conditions Used**: Yes
- **Trigger Condition Logic**: Filter out malformed requests or non-essential data types at the entry point to preserve API quota for critical operations.
- **Polling vs Event-Based**: Event-based is preferred to minimize unnecessary "empty" calls that consume overhead.
- **How Unnecessary Runs Are Avoided**: Implementation of a "Circuit Breaker" pattern at the trigger level; if the downstream service is known to be down or heavily throttled, the flow stops processing new events.

## 3. End-to-End Flow Narrative
The flow begins when an upstream event is captured. Instead of immediately pushing the data to the target service, the flow evaluates the current "throttle state."

- **Initial Check**: The flow checks if a "Cool Down" flag exists in the cache. If active, the flow enters a wait state or moves the message to a retry queue.
- **Execution**: If the service is available, the flow attempts the primary API action.
- **Decision Point (Status Code)**:
    - **Success (2xx)**: The flow completes and logs the successful transaction.
    - **Throttled (429)**: The flow extracts the `Retry-After` header value. It then enters a "Do Until" loop or a delayed retry logic based on the service's recommendation.
    - **Server Error (5xx)**: The flow initiates an exponential backoff strategy.
- **Conclusion**: The flow ends either with a successful data delivery or by moving the message to a Dead Letter Queue (DLQ) after maximum retry attempts are exhausted.

## 4. Key Actions and Connectors
Documenting actions that manage the flow's resilience.

| Action Name | Connector | Purpose | Key Inputs | Key Outputs | Why This Action |
|------------|-----------|---------|------------|-------------|----------------|
| HTTP Request | HTTP | Primary data delivery | Method, URL, Body | Status Code, Headers | Standard interface for REST APIs. |
| Delay | Built-in | Pauses execution | `Retry-After` value | N/A | Prevents "spamming" a throttled service. |
| Parse Headers | Data Operation | Extracting throttle metadata | Response Headers | `retryAfterSeconds` | Necessary to know exactly how long to wait. |
| Update Cache | Redis/SQL | Setting the "Circuit Breaker" | Service Name, TTL | Success Flag | Prevents other concurrent runs from hitting the service. |

## 5. Control Logic and Flow Structure
- **Conditions Used**: To check for specific HTTP status codes (429, 403, 503).
- **Switch Statements**: Used to handle different response types (Success, Temporary Failure, Permanent Failure).
- **Loops (Apply to each / Do until)**: **Do Until** is used for retry logic, continuing until a 200 OK is received or the max retry count is hit.
- **Nested Loops**: No. Nested loops significantly increase execution time and complexity in throttling scenarios.
- **Parallel Branches**: Used for logging and telemetry in parallel with the main retry logic to ensure performance isn't degraded by logging latency.
- **Scope Usage**: "Try-Catch-Finally" blocks are used to encapsulate the API call and its subsequent error handling.

## 6. Error Handling and Monitoring
- **Anticipated Failure Scenarios**: API Rate Limit Exceeded (429), Connection Timeout, Service Unavailable (503).
- **Try Scope Logic**: Contains the main API request.
- **Catch Scope Logic**: Specifically filters for 429 errors to calculate wait times.
- **Finally Scope Logic**: Updates the internal dashboard with the final status (Success/Fail) and clears temporary variables.
- **Run After Configuration**: The Catch block is set to run only if the Try block "has failed" or "has timed out."
- **Failure Notification Method**: Adaptive Cards to Microsoft Teams or Slack for persistent 429 states lasting > 15 minutes.
- **Logging Strategy**: Log the `x-request-id` and the `Retry-After` duration for auditability.
- **How to Debug a Failed Run**: Review the "Outputs" of the HTTP action to see the specific header returned by the service provider.

## 7. Data Handling and Expressions
- **Variables Used**: `varRetryCount` (Integer), `varIsThrottled` (Boolean), `varWaitTime` (Integer).
- **Key Expressions**: 
    - `outputs('HTTP_Action')?['headers']?['Retry-After']`: To get the wait duration.
    - `add(int(variables('varRetryCount')), 1)`: To increment the loop counter.
    - `mul(pow(2, variables('varRetryCount')), 1000)`: To calculate exponential backoff.
- **Data Operations (Select / Filter array / Compose)**: **Compose** is used to store the calculated delay time before passing it to the Delay action.
- **Why Expressions Were Used Instead of Actions**: Expressions allow for complex math (exponential backoff) that standard actions cannot perform natively.

## 8. Performance and Scalability
- **Known Bottlenecks**: The "Delay" action holds a flow execution slot, which can lead to "Flow Exhaustion" if too many instances are waiting.
- **Loop Optimization Strategy**: Use a "Sequential" loop rather than parallel if the service is already throttled.
- **Pagination Handling**: If a GET request is throttled during pagination, the "Next Link" must be preserved and retried.
- **Concurrency Control**: **Critical.** Limit the flow's concurrency (e.g., to 1 or 5) to prevent the flow itself from causing the throttling.
- **What Breaks at Higher Data Volumes**: The internal queue of the orchestration engine may overflow if the "Delay" periods exceed the incoming request rate.
- **Redesign Approach for Scale**: Move from a "Delay" action inside a flow to a "Message Queue with Visibility Timeout" (e.g., Azure Service Bus) to offload the waiting process.

## 9. Security and Governance
- **Connection Type (Personal / Service Account)**: Service Account (Principal) with specific "Rate Limit" scopes.
- **Environment Strategy**: Use dedicated environments for high-throughput flows to isolate resource consumption.
- **Secrets Handling**: API Keys and Client Secrets must be retrieved from a Secure Vault (e.g., Azure Key Vault) and not hardcoded.
- **DLP Considerations**: Ensure the HTTP connector is allowed to communicate only with authorized endpoints.
- **Access Control Notes**: Only "Integration Admins" should have permission to modify concurrency settings.

## 10. Testing and Validation
- **Test Scenarios Covered**: 
    1. Immediate success.
    2. Single 429 with successful retry.
    3. Multiple 429s leading to DLQ.
    4. 503 Service Unavailable.
- **Edge Cases Considered**: Service returns a 429 but *no* `Retry-After` header (default to a standard 60-second wait).
- **Failure Testing**: Using a Mock API (like Postman Mock Server) to intentionally return 429 status codes.
- **Rerun / Recovery Strategy**: Flows that end in the DLQ can be resubmitted manually or via a "Replay" script once the service limit resets.

## 11. Interview Question Mapping
- **Explain This Flow in 2â€“3 Minutes**: "This is a resilience pattern designed to handle API limits. It uses a Try-Catch structure to detect 429 errors, extracts the 'Retry-After' header, and implements a controlled delay before retrying, ensuring we don't get blocked by the provider."
- **How Failures Are Handled**: "We use a combination of status code checks and exponential backoff. If a service is consistently down, we use a circuit breaker to stop requests and alert the team."
- **How Performance Is Optimized**: "By using Concurrency Control to limit how many instances run at once, we stay under the service's threshold and avoid the 429 state entirely."
- **One Trade-Off Made**: "We traded 'Real-time Processing' for 'Reliability.' By introducing delays and concurrency limits, the data might take a few minutes longer to arrive, but we guarantee it won't be dropped."

## 12. Lessons Learned
- **Initial Issues**: Initially, we used a simple "Retry Policy" on the action, but it wasn't sophisticated enough to handle long wait times (over 2 minutes).
- **Improvements Made**: Switched to a manual "Do Until" loop with a "Delay" action to handle wait times of up to several hours if necessary.
- **What I Would Do Differently Now**: I would implement a centralized "Throttling Registry" (like a Redis cache) so that multiple different flows can share the same knowledge about a service's current health.

> [!IMPORTANT]
> Always check the specific vendor documentation for throttling. Some services count "429" hits against your total quota, meaning aggressive retrying can lead to a permanent ban.

> [!TIP]
> When dealing with Microsoft Graph or SharePoint APIs, the `Retry-After` header is usually in seconds. Always convert this to an integer before passing it to a delay function.