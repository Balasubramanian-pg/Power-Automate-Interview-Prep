# 2. Defining Flow Resiliency

## 1. Flow Overview
- **Flow Name**: Enterprise Resiliency & Error Handling Framework
- **Business Problem Statement**: Automated processes often fail due to transient network issues, API rate limits, or unexpected data formats. Without a resiliency framework, these failures require manual intervention, lead to data inconsistency, and increase operational overhead.
- **Business Impact / Value**: Ensures 99.9% process continuity, reduces "Mean Time to Repair" (MTTR), prevents duplicate data processing, and provides clear audit trails for compliance.
- **Trigger Type**: Automated (Pattern-agnostic)
- **Trigger Source**: Multi-source (Dataverse, SQL, Webhooks, or Service Bus)
- **Systems / Connectors Involved**: Cloud Orchestrator (Power Automate/Logic Apps), Monitoring Tools (Application Insights/Log Analytics), Notification Services.
- **Expected Run Frequency**: High-volume, mission-critical execution.
- **Estimated Data Volume**: Variable; designed to scale from single records to batch processing.

## 2. Trigger Design
- **Trigger Connector & Action**: Standardized Event Triggers (e.g., "When a row is added, modified or deleted").
- **Why This Trigger Was Chosen**: Event-based triggers are preferred over polling to reduce latency and minimize unnecessary compute costs.
- **Trigger Conditions Used**: Yes.
- **Trigger Condition Logic**: `@not(equals(triggerOutputs()?['body/status'], 'Processed'))` — used to prevent infinite loops and ensure the flow only reacts to actionable data states.
- **Polling vs Event-Based**: Event-based for real-time responsiveness; Polling only used as a fallback for legacy systems.
- **How Unnecessary Runs Are Avoided**: Implementation of server-side filtering (OData) and trigger conditions to ensure the flow only instantiates when specific business criteria are met.

## 3. End-to-End Flow Narrative
The flow follows a structured "Try-Catch-Finally" architectural pattern to ensure resiliency.

- **What happens when the flow is triggered?** The flow initializes by capturing the trigger metadata and setting up a "Try" Scope. This scope contains the core business logic, including data transformation and external system calls.
- **What are the key decision points?** The flow evaluates the success of each critical action. If an API call fails with a transient error (e.g., 429 or 502), the built-in retry policy engages. If a terminal error occurs (e.g., 400 or 403), the "Try" scope fails immediately.
- **How does the flow end?** If the "Try" scope succeeds, the flow proceeds to the "Finally" scope to log a success status. If the "Try" scope fails, the "Catch" scope intercepts the error, logs the technical details, sends an alert, and gracefully terminates the flow with a "Failed" status to ensure visibility in the dashboard.

> [!IMPORTANT]
> Resiliency is not just about catching errors; it is about ensuring the system returns to a known good state.

## 4. Key Actions and Connectors
Documenting actions that define the resiliency posture.

| Action Name | Connector | Purpose | Key Inputs | Key Outputs | Why This Action |
|------------|-----------|---------|------------|-------------|----------------|
| Scope_Try | Control | Encapsulates main logic | N/A | Success/Failure | Groups actions for collective error handling. |
| Scope_Catch | Control | Error interception | Error Details | Error Log ID | Executes only if Scope_Try fails. |
| HTTP_With_Retry | HTTP | External Integration | URI, Method, Retry Policy | Status Code, Body | Configured with Exponential Backoff for stability. |
| Compose_ErrorMsg | Data Ops | Parse error JSON | `result('Scope_Try')` | Clean Error String | Extracts specific failure reasons for logging. |
| Terminate_Failed | Control | Final Status | "Failed" | N/A | Ensures the flow run is marked correctly in history. |

## 5. Control Logic and Flow Structure
- **Conditions Used**: Used for data validation (e.g., checking if a record exists before updating).
- **Switch Statements**: Used for routing based on error types or business categories.
- **Loops (Apply to each / Do until)**: Used for batch processing.
- **Nested Loops**: No. Nested loops are avoided to prevent performance degradation; "Filter Array" is used instead.
- **Parallel Branches**: Used to log errors to multiple destinations (e.g., SQL and Teams) simultaneously.
- **Scope Usage**: Mandatory. Scopes define the boundaries for Try, Catch, and Finally blocks.

## 6. Error Handling and Monitoring
- **Anticipated Failure Scenarios**: API Throttling (429), Timeout (504), Authentication expiration, and Schema drift.
- **Try Scope Logic**: Contains all "Happy Path" actions.
- **Catch Scope Logic**: Configured to run only if the Try Scope has "Timed Out" or "Failed". It parses the `result()` of the Try scope to find the specific failing action.
- **Finally Scope Logic**: Runs regardless of success or failure to perform cleanup or final logging.
- **Run After Configuration**: The Catch scope is set to "Run after Scope_Try has failed or timed out."
- **Failure Notification Method**: Adaptive Cards sent to a dedicated IT Support Teams channel and an entry in an Azure Log Analytics workspace.
- **Logging Strategy**: Centralized logging using a "Log Entry" child flow to ensure telemetry is captured outside the main process.
- **How to Debug a Failed Run**: Review the "Catch" scope inputs to see the exact JSON error returned by the failing action.

> [!TIP]
> Use the `result('Scope_Name')` expression in your Catch block to programmatically identify which action failed without checking every single step manually.

## 7. Data Handling and Expressions
- **Variables Used**: `varIsSuccess` (Boolean), `varErrorDetails` (String), `varRetryCount` (Integer).
- **Key Expressions**: 
    - `coalesce()`: To handle null values in optional fields.
    - `actions('Action_Name')?['outputs']?['body']`: To safely access dynamic content.
- **Data Operations (Select / Filter array / Compose)**: "Filter Array" is used to isolate failed items in a batch. "Compose" is used for complex string building to avoid variable overhead.
- **Why Expressions Were Used Instead of Actions**: Expressions are used to reduce the "Action Count" per run, which improves performance and stays within entitlement limits.

## 8. Performance and Scalability
- **Known Bottlenecks**: Large "Apply to each" loops without concurrency enabled.
- **Loop Optimization Strategy**: Enable "Concurrency Control" on loops where sequence doesn't matter.
- **Pagination Handling**: Enabled on all "List" actions to ensure the flow can handle datasets exceeding the default 100-record limit.
- **Concurrency Control**: Set to 20-50 depending on the downstream system's rate limits.
- **What Breaks at Higher Data Volumes**: API limits (Request per minute) and the 100MB message size limit.
- **Redesign Approach for Scale**: Transition to a "Splitter-Pattern" where a parent flow breaks data into chunks and calls multiple child flows.

## 9. Security and Governance
- **Connection Type (Personal / Service Account)**: Service Account (Principal-based) is mandatory for production.
- **Environment Strategy**: Development -> UAT -> Production (Managed Environments).
- **Secrets Handling**: All sensitive keys are retrieved from Azure Key Vault; "Secure Inputs/Outputs" is enabled on sensitive actions.
- **DLP Considerations**: Ensure the "HTTP" connector and "Business Data" connectors (e.g., SQL) are in the same data group.
- **Access Control Notes**: Only the Service Account and the DevOps team have "Owner" permissions.

> [!WARNING]
> Never hardcode credentials or API keys directly in the flow actions. Always use Environment Variables or Key Vault.

## 10. Testing and Validation
- **Test Scenarios Covered**: Success path, Invalid Data path, System Offline path.
- **Edge Cases Considered**: Empty arrays, null values in required fields, and maximum character limits.
- **Failure Testing**: Manually forcing a 401 error by temporarily changing a connection string to verify the Catch block triggers.
- **Rerun / Recovery Strategy**: Flows are designed to be **idempotent**. If a flow is rerun, it checks if the work was already completed to avoid duplicate processing.

## 11. Interview Question Mapping
- **Explain This Flow in 2–3 Minutes**: This is a standardized resiliency framework designed to handle enterprise-level integrations. It uses a Try-Catch-Finally pattern to ensure that if any step fails, the error is caught, logged, and reported, while the system remains in a consistent state.
- **How Failures Are Handled**: Failures are handled via "Run After" configurations on Scopes. We capture the error metadata using the `result()` expression, which allows us to provide specific debugging info to the support team.
- **How Performance Is Optimized**: We use Pagination, Concurrency Control, and Filter Arrays to minimize the number of actions executed and maximize throughput.
- **One Trade-Off Made**: We chose to use a "Child Flow" for logging. While this adds a slight overhead to the run time, it ensures that our logging logic is reusable across all flows in the environment.

## 12. Lessons Learned
- **Initial Issues**: Early versions lacked idempotency, leading to duplicate emails being sent when a flow was restarted after a partial failure.
- **Improvements Made**: Added a "Check for Existing Record" step at the beginning of the Try block.
- **What I Would Do Differently Now**: I would implement "Circuit Breaker" logic to automatically disable the flow if it fails more than 10 times in a row, preventing API lockout.

> [!CAUTION]
> Without idempotency, a resilient flow that retries can become a "denial of service" attack on your own database. Always ensure your logic can handle being run twice.