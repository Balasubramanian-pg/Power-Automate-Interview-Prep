# 33. Aggregated Status of Scopes

## 1. Flow Overview
- **Flow Name**: Aggregated Status of Scopes Pattern
- **Business Problem Statement**: In complex automation workflows containing multiple independent modules (Scopes), a single failure often causes the entire flow to terminate prematurely. Business requirements frequently demand that all modules attempt execution and that a consolidated report of successes and failures be generated at the end, rather than stopping at the first error.
- **Business Impact / Value**: Increases process resilience by allowing non-dependent tasks to complete despite isolated failures. It provides granular visibility into which specific business logic units failed, reducing MTTR (Mean Time To Repair) and improving auditability.
- **Trigger Type**: Automated / Instant / Scheduled (Implementation Agnostic)
- **Trigger Source**: Generic (e.g., Power Automate, Logic Apps, or custom orchestrator)
- **Systems / Connectors Involved**: Control Flow (Scopes), Variables, Data Operations, Notification Services (Email/Teams/Slack).
- **Expected Run Frequency**: High-volume enterprise workflows.
- **Estimated Data Volume**: Low (Metadata-heavy, not payload-heavy).

## 2. Trigger Design
- **Trigger Connector & Action**: Dependent on the specific business use case (e.g., "When a file is created" or "Recurrence").
- **Why This Trigger Was Chosen**: The pattern is a logic wrapper; the trigger is chosen based on the entry point of the data being processed.
- **Trigger Conditions Used**: No (Typically handled within the logic).
- **Trigger Condition Logic**: N/A.
- **Polling vs Event-Based**: Event-based is preferred for real-time status aggregation.
- **How Unnecessary Runs Are Avoided**: Standard trigger filters apply; however, the pattern itself is designed to handle the *execution* phase rather than the *trigger* phase.

## 3. End-to-End Flow Narrative
The flow begins by initializing a "Status Collection" array. The core business logic is partitioned into multiple **Scopes** (e.g., Scope_Process_Invoices, Scope_Update_CRM, Scope_Notify_Finance). These scopes may run sequentially or in parallel.

Each scope is followed by a logic gate (or uses a "Run After" configuration) that captures the outcome of that scope. Regardless of whether a scope succeeds or fails, the flow appends a JSON object to the Status Collection array containing the Scope Name, Status (Succeeded/Failed), and Error Message (if applicable).

After all functional scopes have executed, a final **Aggregator Scope** runs. This scope uses a Filter Array operation to identify any "Failed" entries in the collection. Based on the count of failures, the flow determines its final state:
1. If failures = 0: Mark process as Success.
2. If failures > 0: Send a consolidated error report and terminate with a "Failed" or "Timed Out" status to notify the platform orchestrator.

> [!IMPORTANT]
> The Aggregator Scope must be configured to run even if previous scopes have failed (using "Has Failed" or "Is Skipped" run-after settings) to ensure the summary is always generated.

## 4. Key Actions and Connectors
Documenting the structural actions that enable aggregation.

| Action Name | Connector | Purpose | Key Inputs | Key Outputs | Why This Action |
|------------|-----------|---------|------------|-------------|----------------|
| Initialize Status Array | Variables | Stores results of all scopes. | Array Type | Empty Array | Essential for collecting metadata across the run. |
| Functional Scope | Control | Groups related business logic. | Logic Actions | Success/Failure state | Provides a boundary for error handling. |
| Append Status | Variables | Records the outcome of a scope. | Scope Name, Status | Updated Array | Captures state before the flow moves to the next block. |
| Filter Failures | Data Operations | Isolates errors from the collection. | Status Array | Array of Errors | Allows for conditional logic based on total failure count. |
| Final Result | Control | Sets the ultimate flow status. | Failure Count | Final Status Code | Ensures the platform reflects the true state of the process. |

## 5. Control Logic and Flow Structure
- **Conditions Used**: Used at the end to check if the "Failure Array" is empty.
- **Switch Statements**: Occasionally used within the aggregator to handle different severity levels of failures.
- **Loops (Apply to each / Do until)**: Used if the scopes are processing a collection of items where each item needs an individual status.
- **Nested Loops**: No; nesting is avoided to maintain readability and prevent performance degradation.
- **Parallel Branches**: Highly recommended for independent scopes to reduce total execution time.
- **Scope Usage**: Primary structural element. Scopes act as "Try" blocks, while the actions immediately following them act as "Catch/Finally" blocks.

## 6. Error Handling and Monitoring
- **Anticipated Failure Scenarios**: API timeouts, data validation errors, or downstream system unavailability.
- **Try Scope Logic**: Contains the actual business logic.
- **Catch Scope Logic**: Triggered only on failure; captures the error message using expressions like `result('Scope_Name')`.
- **Finally Scope Logic**: Appends the status to the array regardless of success or failure.
- **Run After Configuration**: The "Append Status" action is often set to run after the Scope has "Succeeded", "Failed", or "Timed Out".
- **Failure Notification Method**: A single, consolidated email or dashboard update containing the full list of failed scopes.
- **Logging Strategy**: Log the entire Status Collection array to a centralized logging system (e.g., Azure Application Insights or a SharePoint List).
- **How to Debug a Failed Run**: Examine the "Filter Failures" output to see exactly which scope failed and the error details captured in the array.

> [!TIP]
> Use the `result('Scope_Name')` expression to programmatically access the error details of a scope without needing complex nested conditions.

## 7. Data Handling and Expressions
- **Variables Used**: `varStatusCollection` (Array), `varCurrentError` (String).
- **Key Expressions**: 
    - `length(body('Filter_Failures'))`: To count errors.
    - `item()?['Status']`: To access properties within the filter.
    - `result('Scope_Name')`: To retrieve the JSON output of all actions within a scope.
- **Data Operations (Select / Filter array / Compose)**: "Filter Array" is used to separate "Succeeded" from "Failed" entries. "Compose" is used to format the final HTML/Markdown table for notifications.
- **Why Expressions Were Used Instead of Actions**: Expressions like `length()` are more efficient than using a loop with a counter variable to determine if errors occurred.

## 8. Performance and Scalability
- **Known Bottlenecks**: Large numbers of scopes in a single flow can lead to UI lag in the designer.
- **Loop Optimization Strategy**: If processing items in a loop, use the "Concurrent Control" setting to speed up execution.
- **Pagination Handling**: N/A (Pattern specific).
- **Concurrency Control**: Parallel branches are used to execute independent scopes simultaneously, significantly reducing the critical path.
- **What Breaks at Higher Data Volumes**: If the Status Array grows too large (thousands of entries), the "Append to variable" action may become a bottleneck.
- **Redesign Approach for Scale**: For extremely high-volume status tracking, move the status logging to an external database (e.g., SQL or Dataverse) instead of an in-memory array.

## 9. Security and Governance
- **Connection Type (Personal / Service Account)**: Service Account is mandatory for production logging and notifications.
- **Environment Strategy**: Deploy via Solutions to ensure environment variables handle notification recipients.
- **Secrets Handling**: Error messages captured in the status array should be sanitized to ensure no PII or credentials are logged.
- **DLP Considerations**: Ensure the "Variables" and "Data Operations" connectors are permitted in the environment policy.
- **Access Control Notes**: Only administrators should have access to the run history, as the aggregated status may contain sensitive business logic outcomes.

## 10. Testing and Validation
- **Test Scenarios Covered**:
    1. All scopes succeed (Verify "Success" notification).
    2. One scope fails, others succeed (Verify "Partial Success" report).
    3. All scopes fail (Verify "Critical Failure" notification).
- **Edge Cases Considered**: Scope timeouts, empty input data, and "Skipped" scopes due to upstream logic.
- **Failure Testing**: Manually forcing a "Terminate" or "Divide by Zero" error inside a scope to ensure the aggregator catches it.
- **Rerun / Recovery Strategy**: The aggregated report should include the "Run ID," allowing admins to quickly locate and rerun the specific failed instance.

## 11. Interview Question Mapping
- **Explain This Flow in 2â€“3 Minutes**: "This is a resilient design pattern where I wrap business logic in Scopes and use a 'Run After' configuration to capture the outcome of each. Instead of the flow failing immediately, I collect all statuses into an array and use a Filter Array operation at the end to generate a consolidated report. This ensures that one minor failure doesn't stop the entire process."
- **How Failures Are Handled**: "Failures are caught by the action following a scope. I use the `result()` expression to extract the error message and append it to a tracking variable, allowing the flow to continue to the next task."
- **How Performance Is Optimized**: "I use parallel branches for independent scopes and avoid unnecessary variables inside loops, preferring Data Operations like 'Select' or 'Filter Array' for final status aggregation."
- **One Trade-Off Made**: "The trade-off is complexity in the designer. While it provides better error handling, it requires more initial setup and careful naming of scopes to ensure expressions remain maintainable."

## 12. Lessons Learned
- **Initial Issues**: Initially, I didn't use the "Timed Out" run-after setting, which caused the aggregator to be skipped when a scope hung.
- **Improvements Made**: Switched from multiple "If" conditions to a single "Filter Array" on a JSON collection, which made the flow much cleaner and easier to maintain.
- **What I Would Do Differently Now**: I would implement a standardized JSON schema for the status objects to make it easier to push this data into Power BI for long-term trend analysis of flow reliability.

> [!CAUTION]
> Always rename your Scopes *before* writing expressions that reference them. Renaming a scope later will break any `result('Old_Name')` expressions.