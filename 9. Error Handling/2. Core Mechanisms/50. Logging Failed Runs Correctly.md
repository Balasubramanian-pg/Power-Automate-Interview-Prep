# 50. Logging Failed Runs Correctly

## 1. Flow Overview
- **Flow Name**: Standardized Error Logging Framework (SELF)
- **Business Problem Statement**: Native execution logs in cloud orchestration platforms are often ephemeral (e.g., 28–90 days), difficult to query at scale, and do not provide proactive insights into recurring business logic failures.
- **Business Impact / Value**: Centralizing failure logs enables long-term trend analysis, reduces Mean Time to Repair (MTTR) by providing immediate context, and ensures compliance with audit requirements for critical business processes.
- **Trigger Type**: Automated (Sub-flow/Child Flow) or Pattern-based (Try-Catch blocks)
- **Trigger Source**: Parent Flow Failure Event
- **Systems / Connectors Involved**: SQL Server / Azure Monitor / Dataverse (Log Store), Microsoft Teams / Outlook (Notifications), Cloud Orchestrator (Power Automate/Logic Apps).
- **Expected Run Frequency**: Only upon failure of production workflows.
- **Estimated Data Volume**: Low (Metadata and error strings only).

## 2. Trigger Design
- **Trigger Connector & Action**: Manual / HTTP Request (when used as a Child Flow) or "Run After" configuration (when used as an inline pattern).
- **Why This Trigger Was Chosen**: A centralized Child Flow trigger allows multiple parent flows to log errors to a single repository, ensuring a "Single Source of Truth" for system health.
- **Trigger Conditions Used**: No.
- **Trigger Condition Logic**: N/A (The logic is handled by the "Run After" settings in the parent flow).
- **Polling vs Event-Based**: Event-Based (Triggered immediately upon parent failure).
- **How Unnecessary Runs Are Avoided**: The logging mechanism is only invoked if the "Try" scope of a parent process fails or times out.

## 3. End-to-End Flow Narrative
The process follows the industry-standard **Try-Catch-Finally** pattern to ensure that no failure goes unrecorded.

- **Triggering**: When an action within the "Try" scope of a parent flow fails, the "Catch" scope is initiated via the "Run After" configuration (set to run only if the Try scope has failed or timed out).
- **Data Extraction**: The flow uses the `result()` expression to parse the "Try" scope. It filters the array of actions to find the specific action(s) that returned a status of 'Failed'.
- **Context Gathering**: The flow captures the Run ID, Environment ID, Flow Name, Timestamp, and the specific Error Message/Code provided by the connector.
- **Logging**: This metadata is written to a persistent data store (e.g., a SQL table or Azure Log Analytics).
- **Notification**: A formatted alert is sent to the technical support team via Teams or Email, including a direct link to the specific failed run for immediate debugging.
- **Termination**: The flow ends by explicitly failing the "Catch" scope or using a "Terminate" action to ensure the overall flow status is correctly reported as "Failed" in the dashboard.

## 4. Key Actions and Connectors
Document only the actions that influence logic, performance, or reliability.

| Action Name | Connector | Purpose | Key Inputs | Key Outputs | Why This Action |
|------------|-----------|---------|------------|-------------|----------------|
| Scope (Try) | Control | Encapsulates main logic | N/A | Success/Failure | Groups actions for unified error handling. |
| Scope (Catch) | Control | Handles failure logic | N/A | Error Metadata | Configured to "Run After" Try has failed. |
| Filter Array | Data Ops | Identifies failed action | `result('Try_Scope')` | Failed Action Object | Isolates the exact cause of failure from the scope. |
| Compose (Error) | Data Ops | Extracts Error Message | `item()?['error']?['message']` | Clean Error String | Simplifies the raw JSON into a readable format. |
| Insert Row | SQL/Dataverse | Persistent Logging | RunID, FlowName, Error | Log ID | Ensures logs survive past the platform's retention period. |

## 5. Control Logic and Flow Structure
- **Conditions Used**: Used within the Catch block to check if the error is "Transient" (retriable) or "Fatal" (requires immediate human intervention).
- **Switch Statements**: Often used to route notifications to different teams based on the `FlowName` or `Category`.
- **Loops (Apply to each / Do until)**: Used to iterate through the `result()` array of the Try scope to find all failed actions.
- **Nested Loops**: No.
- **Parallel Branches**: Used to log to a database and send a notification simultaneously to reduce total execution time.
- **Scope Usage**: **Mandatory.** The entire pattern relies on "Try", "Catch", and "Finally" scopes.

## 6. Error Handling and Monitoring
- **Anticipated Failure Scenarios**: Logging service is down; Database connection failure; Permissions issues with the service account.
- **Try Scope Logic**: Contains the actual business logic (e.g., processing an invoice).
- **Catch Scope Logic**: Contains the logging and notification logic.
- **Finally Scope Logic**: Used for cleanup tasks (e.g., closing connections or updating a "Processing" flag to "Error") that must run regardless of success or failure.
- **Run After Configuration**: The Catch scope is set to run if the Try scope: **Has Failed, Has Timed Out.**
- **Failure Notification Method**: Adaptive Cards in Microsoft Teams or HTML-formatted emails.
- **Logging Strategy**: Externalized logging to a SQL Database or Azure Application Insights.
- **How to Debug a Failed Run**: Access the "Catch" scope, look at the "Filter Array" output to see the raw JSON of the failed action in the "Try" scope.

> [!IMPORTANT]
> Always ensure the "Catch" scope itself is monitored. If the logging mechanism fails, you may lose visibility into the original error.

## 7. Data Handling and Expressions
- **Variables Used**: `varErrorDetails` (String), `varIsCritical` (Boolean).
- **Key Expressions**: 
    - `result('Try_Scope_Name')`: Returns an array of all actions in the scope.
    - `workflow()?['run']?['name']`: Retrieves the unique Run ID.
    - `actions('Action_Name')?['outputs']?['body']`: Retrieves specific error details from a known action.
- **Data Operations (Select / Filter array / Compose)**: **Filter Array** is critical to extract only actions where `status` equals 'Failed'.
- **Why Expressions Were Used Instead of Actions**: Expressions like `result()` are the only way to programmatically access the status of multiple actions within a scope without hardcoding every possible failure point.

## 8. Performance and Scalability
- **Known Bottlenecks**: Writing to a slow SQL database can add latency to the error-handling process.
- **Loop Optimization Strategy**: Use `Filter Array` instead of an `Apply to each` with a `Condition` to find failed actions; it is significantly faster.
- **Pagination Handling**: N/A for logging.
- **Concurrency Control**: If logging to a single file (like Excel), concurrency must be disabled. If using SQL/Dataverse, high concurrency is supported.
- **What Breaks at Higher Data Volumes**: If a system-wide outage occurs, hundreds of flows may trigger the logging flow simultaneously, potentially throttling the notification connector (e.g., Outlook limits).
- **Redesign Approach for Scale**: Use an Azure Function or an Event Hub to ingest logs asynchronously if volume exceeds 100 failures per minute.

## 9. Security and Governance
- **Connection Type (Personal / Service Account)**: **Service Account** is mandatory. Logging should not depend on an individual user's credentials.
- **Environment Strategy**: The logging flow should exist in every environment (Dev, Test, Prod) or be a global "Shared Service" flow.
- **Secrets Handling**: Ensure that the `result()` expression does not inadvertently log sensitive inputs (like passwords or PII) into the database.
- **DLP Considerations**: The logging connector (SQL/Azure) must be in the same DLP policy bucket as the business connectors.
- **Access Control Notes**: The log database should be Read-Only for developers and Read-Write for the Service Account.

> [!WARNING]
> Be cautious when logging the `outputs` of a failed action. If the action was processing sensitive data (e.g., SSNs), that data might be present in the error body.

## 10. Testing and Validation
- **Test Scenarios Covered**: 
    - Forced failure (e.g., divide by zero).
    - Timeout failure (e.g., setting a HTTP timeout to 1 second).
    - Connector failure (e.g., invalid credentials).
- **Edge Cases Considered**: What happens if the "Catch" block fails? (Solution: Use a "Finally" block or platform-level alerts).
- **Failure Testing**: Manually "Terminating" a flow to ensure the Catch block triggers.
- **Rerun / Recovery Strategy**: The log entry should include a "Resubmit" link (constructed using the Flow URL and Run ID) to allow support staff to retry the run after fixing the issue.

## 11. Interview Question Mapping
- **Explain This Flow in 2–3 Minutes**: "I implement a standardized error handling pattern using Try-Catch scopes. The Try scope holds the logic, and the Catch scope is configured to run only on failure. It uses the `result()` expression to filter for failed actions, extracts the error message, and logs it to a central SQL database while notifying the team via Teams."
- **How Failures Are Handled**: "Failures are caught at the scope level. This prevents the flow from simply stopping and allows us to execute 'graceful' failure logic, such as logging the error and cleaning up data."
- **How Performance Is Optimized**: "I use Data Operations like 'Filter Array' to parse error logs instead of loops, and I ensure the logging process is as lightweight as possible to avoid overhead."
- **One Trade-Off Made**: "I chose to log to an external SQL database rather than just using the native run history. While this adds complexity, the trade-off is worth it for long-term auditability and the ability to query errors across multiple different flows."

## 12. Lessons Learned
- **Initial Issues**: Initially, I logged every action in the flow, which created too much "noise." I switched to filtering only for 'Failed' actions.
- **Improvements Made**: Added a "Run Link" to the notification so developers don't have to search for the specific failure in the run history.
- **What I Would Do Differently Now**: I would implement a "De-duplication" logic to prevent the same error from firing 100 notifications if a loop fails 100 times.

> [!TIP]
> Use the `workflow()` expression to dynamically get the Environment Name. This helps distinguish between a "Dev" failure and a "Prod" failure in your central log table.