# 58. Parallel Action Execution

Canonical documentation for 58. Parallel Action Execution. This document defines concepts, terminology, and standard usage.

## Purpose
Parallel Action Execution addresses the requirement for high-throughput and low-latency processing within automated systems. In complex workflows, executing tasks sequentially often results in inefficient resource utilization and unnecessary delays, particularly when tasks are independent of one another. 

The primary purpose of this topic is to define the mechanisms by which multiple discrete actions can be initiated and processed simultaneously. By decoupling the execution timeline of independent operations, systems can maximize hardware utilization, reduce total wall-clock time for complex processes, and improve the responsiveness of distributed architectures.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative.

## Scope
Clarify what is in scope and out of scope for this topic.

**In scope:**
* **Concurrency vs. Parallelism:** The theoretical distinction and practical application within action execution.
* **Synchronization Primitives:** Logic for managing dependencies and merging results.
* **Resource Management:** Constraints and strategies for distributing load.
* **Error Propagation:** Handling failures in a multi-threaded or multi-process environment.

**Out of scope:**
* **Specific vendor implementations:** (e.g., GitHub Actions syntax, Jenkins Pipeline DSL, or specific cloud provider orchestrators).
* **Hardware-level instruction parallelism:** (e.g., SIMD, CPU pipeline architecture).
* **Low-level thread management:** Specific programming language syntax for threading.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| **Action** | The smallest atomic unit of work defined within a workflow or system. |
| **Concurrency** | The ability of a system to handle multiple actions by overlapping their execution periods (not necessarily at the same instant). |
| **Parallelism** | The simultaneous execution of multiple actions at the same physical instant, typically requiring multiple processing units. |
| **Fan-out** | The process of triggering multiple parallel actions from a single preceding event or state. |
| **Fan-in (Join)** | The process of synchronizing and aggregating the results of multiple parallel actions before proceeding to the next sequential step. |
| **Race Condition** | A flaw where the output of a process is unexpectedly dependent on the sequence or timing of parallel actions. |
| **Idempotency** | The property of an action where it can be applied multiple times without changing the result beyond the initial application. |
| **Orchestrator** | The entity responsible for scheduling, dispatching, and monitoring parallel actions. |

## Core Concepts

### 1. Independence and Dependency
The fundamental prerequisite for parallel execution is the absence of data or state dependencies between actions. If Action B requires the output of Action A, they are dependent and must be executed sequentially. If Action A and Action B are independent, they are candidates for parallelization.

### 2. Determinism
Parallel execution must strive for determinism. Regardless of the order in which parallel tasks complete, the final state of the system (after the Fan-in) should be predictable and consistent.

### 3. Resource Constraints (The Bound)
Parallelism is never infinite. It is constrained by:
* **Compute Bounds:** Available CPU/GPU cycles.
* **I/O Bounds:** Network throughput or disk read/write speeds.
* **Memory Bounds:** Available RAM for maintaining the state of concurrent operations.

### 4. Synchronization and Coordination
Parallel actions require a coordination layer to manage the lifecycle of tasks. This includes "Forking" (splitting the execution path) and "Joining" (re-converging the paths).

## Standard Model

The standard model for Parallel Action Execution is the **Directed Acyclic Graph (DAG)**. 

1.  **Nodes:** Represent individual actions.
2.  **Edges:** Represent dependencies.
3.  **Parallel Layers:** Nodes at the same depth of the graph that share no edges between them can be executed in parallel.

### The Fork-Join Lifecycle
1.  **Trigger:** A condition is met that allows for multiple actions.
2.  **Dispatch:** The orchestrator allocates resources and starts the actions.
3.  **Execution:** Actions run independently.
4.  **Monitoring:** The orchestrator tracks the status (Running, Success, Failure) of each action.
5.  **Aggregation:** Once all (or a required subset) of actions complete, the results are collected.
6.  **Continuation:** The workflow moves to the next sequential stage.

## Common Patterns

### Scatter-Gather
A single request is broadcast to multiple processing units (Scatter). Each unit performs a task, and the results are reassembled into a single response (Gather).

### Throttled Parallelism (Worker Pool)
To prevent resource exhaustion, a fixed number of "workers" are established. Actions are placed in a queue and processed as workers become available, limiting the maximum degree of parallelism.

### Speculative Execution
Starting multiple versions of the same action or different approaches to the same problem in parallel. The first one to succeed is accepted, and the others are terminated.

### Pipeline Parallelism
Parallelizing different stages of a linear process. While Action 1 processes the second item in a batch, Action 2 can simultaneously process the first item.

## Anti-Patterns

### Over-Parallelization
Attempting to run more parallel actions than the underlying infrastructure can support. This leads to "context switching" overhead, where the system spends more time managing tasks than executing them.

### Shared Mutable State
Allowing parallel actions to modify the same global variable or database record without proper locking mechanisms. This is the primary cause of non-deterministic behavior and data corruption.

### The "Silent Failure"
Designing a fan-in mechanism that proceeds if some actions fail without reporting or handling those failures. This leads to incomplete data sets in subsequent steps.

### Hard-Coding Concurrency Limits
Setting a fixed number of parallel threads (e.g., "always run 10") rather than scaling based on available system resources or workload volume.

## Edge Cases

### Partial Success (The "M of N" Problem)
What happens if 8 out of 10 parallel actions succeed? Documentation must define whether the entire operation is a failure, or if the system should proceed with partial results.

### Deadlocks in Parallel Dependencies
Occurs when Action A is waiting for Action B, and Action B is waiting for Action A, or when both are waiting for a shared resource that neither will release.

### Zombie Actions
When a parent process or orchestrator terminates, but the parallel child actions continue to run, consuming resources and potentially causing side effects in later runs.

### Clock Skew
In distributed parallel execution, different nodes may have slightly different system times, which can complicate the ordering of logs or the resolution of "last-write-wins" conflicts.

## Related Topics
* **12. Idempotency Patterns:** Crucial for ensuring that retried parallel actions do not cause side effects.
* **24. Distributed Locking:** Mechanisms for managing shared resources during parallel execution.
* **41. Error Handling and Retry Logic:** Strategies for managing failures in concurrent environments.
* **60. State Machine Design:** How parallel transitions affect the overall state of a system.

## Change Log

| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-16 | Initial AI-generated canonical documentation |