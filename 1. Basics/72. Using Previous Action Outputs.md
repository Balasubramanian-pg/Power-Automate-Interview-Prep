# 72. Using Previous Action Outputs

Canonical documentation for 72. Using Previous Action Outputs. This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of utilizing previous action outputs is to enable data continuity and statefulness within a sequential or directed acyclic graph (DAG) workflow. In automated systems, actions rarely function in isolation; the result of one operation typically serves as the prerequisite or configuration for a subsequent operation. This topic addresses the mechanism by which data is captured, stored, and referenced across the lifecycle of a process execution.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative.

## Scope
Clarify what is in scope and out of scope for this topic.

**In scope:**
* Mechanisms for data persistence between discrete execution steps.
* Referencing syntax logic and variable scoping.
* Data transformation and mapping between output and input schemas.
* Lifecycle management of output data within a single execution context.

**Out of scope:**
* Specific vendor-specific expression languages (e.g., Jinja, jq, or proprietary syntax).
* Long-term data archiving or external database persistence.
* Real-time streaming data protocols.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| **Action** | A discrete, atomic unit of work within a workflow or pipeline. |
| **Output** | The data payload produced by an action upon completion, often consisting of metadata and execution results. |
| **Input** | The parameters or data required by an action to begin execution. |
| **Context** | The shared memory space or object containing all available data from previously executed actions. |
| **Reference** | A pointer or symbolic link used to retrieve a specific piece of data from the context. |
| **Upstream** | Any action that executes prior to the current action in the workflow sequence. |
| **Downstream** | Any action that executes after the current action and may consume its outputs. |
| **Schema** | The structural definition (types, keys, constraints) of the output data. |

## Core Concepts

### Data Linearity and Dependency
The fundamental concept of using previous outputs is the establishment of a functional dependency. If Action B requires an output from Action A, a dependency is created. This dependency dictates execution order and ensures that the required data is materialized before the downstream consumer initializes.

### The Context Object
In most systems, outputs are not passed directly from one action to the next in a "bucket brigade" fashion. Instead, they are appended to a centralized **Context Object** (or State Store). This object acts as the single source of truth for the duration of the execution, allowing any downstream action to query the context for specific upstream data.

### Immutability of Outputs
Once an action completes and its output is committed to the context, that data should be considered immutable. Subsequent actions may transform the data and produce *new* outputs, but they cannot retroactively alter the recorded output of a previous step. This ensures auditability and prevents race conditions.

## Standard Model

The standard model for using previous action outputs follows a **Capture-Store-Reference** lifecycle:

1.  **Capture:** The execution engine monitors the completion of an action. It captures the return code, standard output, and specifically defined "output variables."
2.  **Store:** The engine serializes this data and stores it in the execution context, keyed by a unique identifier (usually the Action ID or Step Name).
3.  **Reference:** A downstream action defines its inputs using a reference syntax (e.g., `{{ actions.step_id.output_key }}`).
4.  **Resolution:** Before the downstream action executes, the engine resolves all references by fetching the values from the context and injecting them into the action's environment.

## Common Patterns

### The Chain Pattern
A simple linear progression where Action B uses the output of Action A, and Action C uses the output of Action B. This is the most common form of data flow.

### The Aggregator Pattern
A single downstream action collects outputs from multiple independent upstream actions to perform a summary task or a "join" operation.

### The Filter/Transformer Pattern
An intermediate action exists solely to take a large output from a previous step (e.g., a raw JSON blob) and format, filter, or reduce it into a specific schema required by the next functional step.

### Conditional Execution
Using the output of a previous action (often a boolean or status code) to determine whether subsequent actions should execute or be skipped.

## Anti-Patterns

### Hard-Coding Identifiers
Relying on volatile or non-deterministic identifiers to reference previous outputs. If an Action ID changes and the reference is not updated, the workflow breaks.

### Deep Coupling
Designing actions that require intimate knowledge of the internal structure of a previous action's raw logs rather than relying on structured, formal output variables.

### The "God" Context
Passing excessively large payloads (e.g., entire binary files or multi-gigabyte datasets) through the context object. This can lead to memory exhaustion. Standard practice is to pass a reference (like a URI or file path) rather than the data itself.

### Ignoring Failure States
Attempting to consume the output of an action that failed or was skipped. Without proper error handling or "default value" logic, this leads to null pointer exceptions or cascading failures.

## Edge Cases

*   **Dynamic Keys:** When an upstream action produces an output where the keys are generated dynamically at runtime, making static referencing impossible. This requires iterative or reflective access patterns.
*   **Name Collisions:** When two actions in a parallel execution branch produce outputs with the same name, potentially overwriting each other if the context is not properly namespaced.
*   **Asynchronous Callbacks:** When an action initiates a long-running process and returns an "In-Progress" token. The "output" in this case is a promise or a handle, and the actual data must be retrieved by a later polling action.
*   **Sensitive Data:** Outputs containing credentials or PII (Personally Identifiable Information). These must be masked in logs while remaining accessible to downstream actions in a secure manner.

## Related Topics
*   **71. Action Input Validation:** How inputs are verified before an action runs.
*   **73. Workflow State Management:** The broader management of data across complex, multi-branch processes.
*   **85. Error Handling and Retries:** How data flow is managed when an upstream action fails.

## Change Log

| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-16 | Initial AI-generated canonical documentation |