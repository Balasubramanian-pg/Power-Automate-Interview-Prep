# 71. Using Trigger Outputs

Canonical documentation for 71. Using Trigger Outputs. This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of Trigger Outputs is to provide the necessary context and data payload from an initiating event to subsequent operations within a workflow or process. In event-driven architectures, a trigger acts as the entry point; however, the mere occurrence of an event is rarely sufficient for complex automation. Trigger Outputs solve the problem of data continuity by capturing stateful information at the moment of invocation and making it accessible for downstream consumption, transformation, and decision-making.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative.

## Scope
Clarify what is in scope and out of scope for this topic.

**In scope:**
* **Data Propagation:** The mechanism by which data moves from the trigger source to the execution environment.
* **Schema Definition:** The structure and typing of data emitted by a trigger.
* **Access Patterns:** How downstream components reference and retrieve trigger-provided data.
* **Lifecycle of Output Data:** The persistence and availability of trigger data during the execution of a process.

**Out of scope:**
* **Specific Vendor Syntax:** Platform-specific expressions (e.g., specific JSONPath implementations or proprietary scripting languages).
* **Trigger Configuration:** The logic used to define *when* a trigger fires (e.g., polling intervals or webhook listeners).
* **Transport Protocols:** The underlying network protocols (HTTP, MQTT, etc.) used to deliver the trigger.

## Definitions
| Term | Definition |
|------|------------|
| Trigger | An event or condition that initiates the execution of a workflow or function. |
| Output Payload | The structured data object emitted by the trigger upon successful invocation. |
| Schema | The formal definition of the data types, keys, and structures present in the trigger output. |
| Downstream Consumer | Any action, function, or logic gate that utilizes data provided by the trigger. |
| Dynamic Content | Data points within the trigger output that vary per execution (e.g., a unique ID or timestamp). |
| Metadata | Data about the trigger event itself (e.g., source IP, arrival time) rather than the primary payload. |

## Core Concepts

### 1. Data Encapsulation
Trigger outputs encapsulate the state of the system at the exact moment the event occurred. This encapsulation ensures that even if the source system changes immediately after the trigger fires, the workflow operates on the "point-in-time" data captured during the invocation.

### 2. Schema Contract
A trigger output operates under a schema contract. Downstream steps rely on the presence of specific keys and data types (strings, integers, booleans, objects). If the trigger source alters its output schema without updating the downstream consumers, the workflow enters a failure state.

### 3. Availability and Scope
Trigger outputs are typically globally scoped within a single execution instance. This means the data remains available to every subsequent step in that specific run, regardless of how many intermediate steps have occurred.

## Standard Model

The standard model for using trigger outputs follows a **Capture-Map-Consume** lifecycle:

1.  **Capture:** The system intercepts an event and parses the incoming data into a standardized internal format (usually JSON or a similar key-value structure).
2.  **Map:** The user or system defines references between the trigger output fields and the input fields of downstream actions. This is often referred to as "Data Binding."
3.  **Consume:** During runtime, the execution engine resolves these references, injecting the actual values from the trigger output into the downstream actions.

### Data Hierarchy
Trigger outputs are generally organized into three layers:
*   **Header/Metadata:** Information about the request (e.g., Request ID, Timestamp).
*   **Body:** The primary data payload (e.g., User details, File contents).
*   **Context:** Environmental data (e.g., Environment name, Triggering user identity).

## Common Patterns

### The Pass-Through Pattern
The most basic pattern where trigger outputs are passed directly to a destination (e.g., a "New Email" trigger passes the "Subject" directly to a "Log Entry" action).

### The Transformation Pattern
Trigger outputs are modified via expressions or functions before being used. Common transformations include string manipulation, mathematical operations, or date reformatting to meet the requirements of the downstream system.

### The Conditional Branching Pattern
Trigger outputs are used as the basis for logic gates. For example, if a trigger output `priority` equals `high`, the workflow follows Path A; otherwise, it follows Path B.

### The Enrichment Pattern
A trigger output provides a unique identifier (e.g., `CustomerID`), which is then used by a subsequent step to fetch more detailed information from a database, effectively "enriching" the initial trigger data.

## Anti-Patterns

### Hard-Coding Volatile References
Relying on specific indices in an array that may change order, rather than using unique keys or identifiers provided in the trigger output.

### Over-Mapping
Mapping every available trigger output field to downstream steps regardless of necessity. This creates "brittle" workflows that are more likely to fail if the source schema changes in areas that are not actually relevant to the process.

### Ignoring Nulls
Failing to account for optional fields in a trigger output. If a downstream step expects a value that the trigger provides as `null` or `undefined`, it may cause unhandled exceptions.

### Security Leakage
Passing sensitive trigger outputs (like API keys or PII) into logs or non-secure downstream systems without masking or encryption.

## Edge Cases

### Large Payload Handling
When a trigger output exceeds the memory limits of the execution environment (e.g., a 500MB file attachment). Standard models often replace the actual data with a "pointer" or "reference URL" to the data rather than the data itself.

### Race Conditions
In scenarios where multiple triggers fire in rapid succession, the output of one trigger must be strictly isolated to its own execution instance to prevent data "bleeding" between runs.

### Schema Evolution (Breaking Changes)
When the source system adds, removes, or renames fields in the trigger output. Robust systems implement versioning for triggers to allow downstream consumers to migrate to new schemas at their own pace.

### Empty Payloads
A trigger that fires successfully but contains an empty body. Systems must be designed to handle "Success-No-Data" scenarios without crashing.

## Related Topics
*   **72. Data Transformation and Mapping:** The logic used to modify trigger outputs.
*   **15. Event-Driven Architecture:** The broader architectural style governing triggers.
*   **44. Error Handling in Workflows:** How to manage failures when trigger outputs are missing or malformed.
*   **09. Webhook Security:** Securing the delivery of trigger payloads.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-16 | Initial AI-generated canonical documentation |